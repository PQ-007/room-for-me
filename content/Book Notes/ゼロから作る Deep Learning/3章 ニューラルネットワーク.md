***Perceptron  ашиглахад***:
- **Олон давхаргатай perceptron**-оор хэцүү бүтэц бүхий тооцооллын урсгалыг загварчилж болно.
- Гэвч **weight, bias** зэрэг параметрүүдийг нь зөв тохируулж өгөх хэрэгтэй.

> **Чухамдаа машин сургах гэж эдгээр параметрийг тохируулах тухай л зүйл.**

Гараар, хүний оролцоотой тохируулж ирсэн бол одоо энэхүү процессыг автоматжуулах хэрэг гарч ирэх бөгөөд нейрон сүлжээ нь үүний шийдэл юм. Тохирох жингийн параметрийг өгөгдөл дотроос өөрөө сурч чадах нь нейрон сүлжээний чухал шинж.

Энэ бүлэгт нейрон сүлжээний үндсэн агуулга, түүний ялган таних үйл явц өрнүүлэх үеийн боловсруулалтын тухай голчлон авч үзнэ. 

## パーセプトロンからニューラルネットワークへ

Нейрон сүлжээ нь өмнө бүлэгт өгүүлсэн Perceptron-тай олон талаараа ижил. 

### ニューラルネットワークの例

Доорх зурагт нейрон сүлжээний жишээг харуулсан байна. Оролтын давхарга, дунд байрлах давхарга (далд давхарга), гаралтын давхарга гэсэн хэсгүүдтэй. Далд давхаргад нейрон нь оролтын болон гаралтын давхаргаас ялгаатай нь харагдахгүй учир ингэж нэрлэсэн. 

![[Neural Network Example.png]]

Хэлбэрийн хувьд нейронуудыг давхарлаж холбосон учир Perceptron-той ижил харагдана.

### パーセプトロンの復習

![[Perceptron Example.png]]
$$
y = \begin{cases}
    0 & (b+w_1 x_1 + w_2 x_2 \leq 0) \\
    1 & (b+w_1 x_1 + w_2 x_2 > 0)
\end{cases}
$$
Perceptron нь дээрх тэгшитгэлийн дагуу ажилладаг. 
Энд $b$ нь bias, $w_1、w_2$ нь жин, $x_1、x_2$ нь оролтын утгууд. Жишээ зурагт bias-ийг дүрсэлж харуулаагүй бөгөөд дараах байдлаар нэмж болно. Яагаад нэг оролтын нейрон нэмж 1 гэж тэмдэглэж байгаа вэ гэвэл сургалт эхлээгүй байхад $b$ = 1  гэж үздэг учраас 1 ээр төлөөлүүлж, цааш сурах явцдаа тохируулж болохуйц хувьсагч гэдгийг нь илэрхийлэх үүднээс $b$ нь жин шиг холбоосон дээр дүрслэгдсэн.

![[Perceptron with bias.png]]

Perceptron-ны тэгшитгэлийг хялбарчилж өөр хэлбэрт бичье. Хялбарчлахын тулд тухайн оролт бүрт бодогдон  гарч буй нийлбэр нь 0-ээс их байх үед 1, эсрэг тохиолдолд 0-ийг гаргах хэсгийг нэг функц болгоё. Энэ функцыг $h(x)$ гээд шинээр бичвэл:

$$
y = h(b + w_1x_1 + w_2x_2)
$$
$$
h(x) = \begin{cases}
    0 & (x \leq 0) \\
    1 & (x > 0)
\end{cases}
$$
Эхний тэгшитгэл нь оролтын дохионуудын нийлбэрийг  $h(x)$ гэдэг функцт оруулж буцааж буй утгыг нь $y$-т авч байгааг илэрхийлнэ. Дараагийн тэгшитгэл нь $h(x)$ гэдэг функц нь оролт нь эерэг үед 1-ийг буцааж, сөрөг үед 0-ийг буцаадаг функц гэдгийг харуулж байна.

### 活性化関数の登場 (Activation function)

Дээр дурдсан $h(x)$ шиг оролтын дохионуудын нийлбэрийг хүлээн авч гаралтын дохионд хувиргадаг функцуудыг **Идэвхжүүлэх функц (Activation function)** гэдэг.
Энэ функц дохионуудын нийлбэрийг хэр зэрэг идэвжүүлэх вэ? Өөрөөр хэр зэрэг 1 гэсэн дохиог цааш дамжуулах вэ? гэдгийг шийдвэрлэх үүрэгтэй.

Илүү цэвэрхэн ойлгомжтойгоор бичвэл:

$$a = b + w_1x_1 + w_2x_2$$
$$y=h(a)$$
болно.
Зургаар харуулвал
![[Perceptron with activation function.png]]

  Энэхүү идэвхжүүлэх функц нь Perceptron-оос нейроны сүлжээ рүү урагшлах гүүр болно.

## 活性化関数

- Perceptron дахь идэвхжүүлэх функц нь **степ функц (step function)** байдаг.
    
- Энэ функц нь оролтын утгыг тодорхой нэг **threshold (閾値)**-оос хамааран 0 эсвэл 1 болгон гаргадаг.
    
- Хэрэв степ функцийн оронд өөр төрлийн активэйшн функц ашиглавал яах вэ?
    
- Үнэндээ, идэвхжүүлэх функцийг степээс өөр (жишээ нь сигмойд, ReLU гэх мэт) болгон өөрчлөх нь **нейрон сүлжээний (ニューラルネットワーク)** рүү орох эхлэл болдог.

### シグモイド関数

$$h(x) = \frac{1}{1+e^{-x}} $$
Сигмойд функцын тэгшигтгэл.
### シグモイド関数とステップ関数の比較

Степ болон Сигмойд функцын ялгааг харицуулая.
![[Pasted image 20250918155836.png]]

Степ функц нь 0, 1 гэсэн хоёрхон утгыг гаргаж чадах бол сигмойд функц нь 0.731,  0.880 зэрэг бутархай утга гаргаж чадна гэдгээрээ ялгаатай. Нейрон сүлжээ үргэлжилсэн бодит утгуудыг гаргаж чаддаг гэсэн үг. 

Одоо ойролцоо байгаа зүйлийн тухайд гэвэл ерөнхийдөө ижил хэлбэрийг гаргаж байгаа. Бага оролт нам утга, их оролт өндөр утга гаргана.

### 非線形関数 (non linear function)

Идэвхжүүлэх функц нь ямар нэгэн оролт ороход ямар нэгэн утга буцаадаг хувиргах хэрэгсэл юм. Ингэхдээ гаралт нь оролттойгоо шууд пропорционал хамааралтай (тодорхой тоогоор үржүүлэн нөгөөгөө гаргадаг, хоёр хэмжигдэхүүний харьцаа тогтмол байх хамаарал) байх үед тухайн функц нь **шулуун функц**, үгүй бол **шулуун биш функц**.
$$(h(x) = cx, \quad c = \text{const}) \quad ? \quad \text{linear function} \quad : \quad \text{non-linear function} $$Нейрон сүлжээнд маань ашиглах идэвхжүүлэх функц нь **шулуун биш функц** байх ёстой.  Хэрвээ **шулуун функц** ашиглачихвал нейроны сүлжээний давхарга нь нэмэгдэх гэдэгт ямар ч ашиггүй болчихдог. Гүнзгийрэн давхарга нэмэгдэх тусам далд давхаргад өмнөх давхаргад хийгдсэнтэй ижил процесс явуулдаг нейрон заавал байж байдаг.
### ReLU (Rectified Linear Unit) 関数

![[Pasted image 20250918201421.png]]

Гаралт нь 0-ээс илүү байвал тэр утгаа буцаана, үгүй бол 0-ийг буцаана.
$$
h(x) = \begin{cases}
	x & (x > 0) \\
    0 & (x \leq 0) 
\end{cases}
$$
```python
def relu(x):
return np.maximum(0, x)
```

## 多次元配列の計算
 
Эдгээр $b, x_1, x_2, ... x_n, w_1, w_2, ... w_n$ үүдийг тооцоолох хэрэг одоо гарч ирнэ. 
Матриц дээр хийгдэх бүхий л үйлдлийг NumPy сан ашиглан хийж болно. 

Жин,  оролтуудыг тус тусад нь нэг матриц болгоод хооронд нь үржүүлчихвэл нийлбэрийг амархан бодно.

![[Pasted image 20250918204825.png]]

```python
>>> X = np.array([1, 2])
>>> X.shape
(2,)
>>> W = np.array([[1, 3, 5], [2, 4, 6]])
>>> print(W)
[[1 3 5]
[2 4 6]]
>>> W.shape
(2, 3)
```

![[Pasted image 20250918204306.png]]

```python
>>> Y = np.dot(X, W)
>>> print(Y)
[ 5 11 17]
```

Арай бодит нейрон сүлжээн дээр дахин жишээ авъя.

![[Pasted image 20250918205421.png]]

....

## 出力層の設計

Нейрон сүлжээг **ялгах (classification)**, болон **багасгах (regression)** гэсэн 2 даалгаварт ашиглаж болно. 

### 恒等関数とソフトマックス関数

**Таних функц (Identity function)** гэж орж ирсэн утгыг тэр чигээр нь гаргадаг функц юм.
Гаралтын давхаргад энэ функцийг ашиглана. 

![[Pasted image 20250918213028.png]]

Ялгаж ангиллах даалгаварт ашигладаг нь харин **Softmax** юм. 

$$ y_k = \frac{e^{a_k}}{\sum \limits_{i=1}^{n} e^{a_i}} $$
Энд гаралтын давхарга нийтдээ **n** ширхэг байна гэж үзвэл, **k** дахь гаралт $y_k$-ийг тооцоолох томъёог үзүүлж байна. Томъёод үзүүлснээр, Softmax функцийн тооны дээд хэсэг нь оролтын дохион $a_k$-ийн экспоненциал, доод хэсэг нь бүх оролтын дохионуудын экспоненциалын нийлбэрээр бүрддэг.

![[Pasted image 20250918214310.png]]

### ソフトマックス関数の実装上の注意

Softmax ашиглах үед анхны томьёо компьютерийн тооцоолж чадахгүй хэт их тоо руу хөтөлдөг. Тиймээс дараах  сайжруулсан томьёог ашигладаг.

$$ y_k = \frac{e^{a_k}}{\sum \limits_{i=1}^{n} e^{a_i}} = \frac{Ce^{a_k}}{C\sum \limits_{i=1}^{n} e^{a_i}} = \frac{e^{a_k+logC}}{\sum \limits_{i=1}^{n} e^{a_i+logC}} = \frac{e^{a_k+C^{'}}}{\sum \limits_{i=1}^{n} e^{a_i+C^{'}}} $$
### ソフトマックス関数の特徴

Softmax-ийн гаралтын утгууд нь 0-ээс 1-ийн хоорондох бутархай тоонууд байх ба тэдгээрийн нийлбэр нь 1-тэй тэнцүү байдаг. Тийм болохоор гаргалтуудыг хувиар илэрхийлэх боломжтой болгодог. Softmax ашиглахад гаралтуудын хоорондыг харьцаа өмнө байснаараа хадгалагддаг. Ийм учраас Softmax ашиглахгүй байсан ч хамгийн их утгатайг нь хариу болгон сонгож чадна. Softmax ашигладаг нь сургалт явуулах үед хэрэгтэй учраас тэр.
### 出力層のニューロンの数

Гаралтын давхаргын нейронуудын тоог шийдэхдээ тухайн даалгаварын шинж чанарт тохируулан тогтоох шаардлагатай. Хэрэв ангилах даалгавар шийдэх гэж байгаа бол гаралтын давхаргын нейронуудын тоог ангилах гэж буй ангиллын тоонд харгалзуулан тохируулах нь түгээмэл. Жишээлбэл, нэгэн оролтын зургийн хувьд, тухайн зураг нь 0–9 хүртэлх тоонуудын аль нэг болохыг таамаглах асуудал буюу **10 төрлийн сонголттой** энэ нөхцөлд гаралтын давхаргын нейронуудыг 10 болгон тохируулна.

![[Pasted image 20250918224522.png]]

**Сургалтын үе шат аль хэдийн дууссан** гэж төсөөлөе. Өөрөөр хэлбэл, моделио сургачихсан бөгөөд сурсан параметрүүд (жингүүд, bias) бидэнд байгаа. Тиймээс дахин сургах шаардлагагүй.

Бид зөвхөн **таамаглах процесс** буюу сурсан моделио ашиглан шинэ, мэдэгдээгүй өгөгдөл дээр ангилал хийх процессыг хэрэгжүүлэх болно.

Энэ таамаглах процессыг мөн **forward propagation (順方向伝播)** гэж нэрлэдэг. Учир нь өгөгдөл нь **сүлжээгээр урагш урсах** байдлаар тооцоологддог:

1. Оролтын дүрс анхны давхаргад орно.
    
2. Давхарга бүр сурсан жингүүд ба идэвхжүүлэх функцуудыг ашиглан гаралт тооцно.
    
3. Эцэст нь гаралтын давхарга тухайн оролт ямар тоо байж болох магадлалын оноонуудыг гаргана.

Товчхондоо:

- **Сургалтын үе шат** = олон жишээгээр моделио сургаж параметрүүдийг тааруулах
    
- **Таамаглах үе шат / Forward propagation** = сурсан моделиор оролтыг урагш дамжуулж гаралтыг авах (зураг дээр ямар тоо байгааг таах).

гэсэн 2 үе шатанд машин сургалтын үйл явц өрнөдөг.

### バッチ処理

.....

## まとめ

- Нейрон сүлжээнд идэвхжүүлэх функц болгон **сигмоид функц**, **ReLU функц** зэрэг зөөлөн өөрчлөгддөг функцуудыг ашигладаг.  
	
- NumPy-ийн олон хэмжээст массивыг зөв ашигласнаар нейрон сүлжээг үр ашигтайгаар хэрэгжүүлэх боломжтой. 
	
- Машин сургалтын асуудлуудыг ерөнхийд нь **регрессийн асуудал** болон **ангиллын асуудал** гэж ангилдаг.  
	
- Гаралтын давхаргад ашиглах идэвхжүүлэх функц нь, регрессийн асуудалд **тэнцүү (identity) функц**, ангиллын асуудалд **Softmax функц** байх нь түгээмэл. 
	
- Ангиллын асуудал дээр гаралтын давхаргын нейронуудын тоог ангилах гэж буй ангийн тоонд тохируулна.  
	
- Оролтын өгөгдлийн бөөгнөрлийг **batch** гэж нэрлэдэг бөгөөд batch нэгжээр таамаглах процессыг хийх нь тооцооллыг хурдан болгодог.