***Perceptron  ашиглахад***:
- **Олон давхаргатай perceptron**-оор хэцүү бүтэц бүхий тооцооллын урсгалыг загварчилж болно.
- Гэвч **weight, bias** зэрэг параметрүүдийг нь зөв тохируулж өгөх хэрэгтэй.

> **Чухамдаа машин сургах гэж эдгээр параметрийг тохируулах тухай л зүйл.**

Гараар, хүний оролцоотой тохируулж ирсэн бол одоо энэхүү процессыг автоматжуулах хэрэг гарч ирэх бөгөөд нейрон сүлжээ нь үүний шийдэл юм. Тохирох жингийн параметрийг өгөгдөл дотроос өөрөө сурч чадах нь нейрон сүлжээний чухал шинж.

Энэ бүлэгт нейрон сүлжээний үндсэн агуулга, түүний ялган таних үйл явц өрнүүлэх үеийн боловсруулалтын тухай голчлон авч үзнэ. 

## パーセプトロンからニューラルネットワークへ

Нейрон сүлжээ нь өмнө бүлэгт өгүүлсэн Perceptron-тай олон талаараа ижил. 

### ニューラルネットワークの例

Доорх зурагт нейрон сүлжээний жишээг харуулсан байна. Оролтын давхарга, дунд байрлах давхарга (далд давхарга), гаралтын давхарга гэсэн хэсгүүдтэй. Далд давхаргад нейрон нь оролтын болон гаралтын давхаргаас ялгаатай нь харагдахгүй учир ингэж нэрлэсэн. 

![[Neural Network Example.png]]

Хэлбэрийн хувьд нейронуудыг давхарлаж холбосон учир Perceptron-той ижил харагдана.

### パーセプトロンの復習

![[Perceptron Example.png]]
$$
y = \begin{cases}
    0 & (b+w_1 x_1 + w_2 x_2 \leq 0) \\
    1 & (b+w_1 x_1 + w_2 x_2 > 0)
\end{cases}
$$
Perceptron нь дээрх тэгшитгэлийн дагуу ажилладаг. 
Энд $b$ нь bias, $w_1、w_2$ нь жин, $x_1、x_2$ нь оролтын утгууд. Жишээ зурагт bias-ийг дүрсэлж харуулаагүй бөгөөд дараах байдлаар нэмж болно. Яагаад нэг оролтын нейрон нэмж 1 гэж тэмдэглэж байгаа вэ гэвэл сургалт эхлээгүй байхад $b$ = 1  гэж үздэг учраас 1 ээр төлөөлүүлж, цааш сурах явцдаа тохируулж болохуйц хувьсагч гэдгийг нь илэрхийлэх үүднээс $b$ нь жин шиг холбоосон дээр дүрслэгдсэн.

![[Perceptron with bias.png]]

Perceptron-ны тэгшитгэлийг хялбарчилж өөр хэлбэрт бичье. Хялбарчлахын тулд тухайн оролт бүрт бодогдон  гарч буй нийлбэр нь 0-ээс их байх үед 1, эсрэг тохиолдолд 0-ийг гаргах хэсгийг нэг функц болгоё. Энэ функцыг $h(x)$ гээд шинээр бичвэл:

$$
y = h(b + w_1x_1 + w_2x_2)
$$
$$
h(x) = \begin{cases}
    0 & (x \leq 0) \\
    1 & (x > 0)
\end{cases}
$$
Эхний тэгшитгэл нь оролтын дохионуудын нийлбэрийг  $h(x)$ гэдэг функцт оруулж буцааж буй утгыг нь $y$-т авч байгааг илэрхийлнэ. Дараагийн тэгшитгэл нь $h(x)$ гэдэг функц нь оролт нь эерэг үед 1-ийг буцааж, сөрөг үед 0-ийг буцаадаг функц гэдгийг харуулж байна.

### 活性化関数の登場 (Activation function)

Дээр дурдсан $h(x)$ шиг оролтын дохионуудын нийлбэрийг хүлээн авч гаралтын дохионд хувиргадаг функцуудыг **Идэвхжүүлэх функц (Activation function)** гэдэг.
Энэ функц дохионуудын нийлбэрийг хэр зэрэг идэвжүүлэх вэ? Өөрөөр хэр зэрэг 1 гэсэн дохиог цааш дамжуулах вэ? гэдгийг шийдвэрлэх үүрэгтэй.

Илүү цэвэрхэн ойлгомжтойгоор бичвэл:

$$a = b + w_1x_1 + w_2x_2$$
$$y=h(a)$$
болно.
Зургаар харуулвал
![[Perceptron with activation function.png]]

  Энэхүү идэвхжүүлэх функц нь Perceptron-оос нейроны сүлжээ рүү урагшлах гүүр болно.

## 活性化関数

- Perceptron дахь идэвхжүүлэх функц нь **степ функц (step function)** байдаг.
    
- Энэ функц нь оролтын утгыг тодорхой нэг **threshold (閾値)**-оос хамааран 0 эсвэл 1 болгон гаргадаг.
    
- Хэрэв степ функцийн оронд өөр төрлийн активэйшн функц ашиглавал яах вэ?
    
- Үнэндээ, идэвхжүүлэх функцийг степээс өөр (жишээ нь сигмойд, ReLU гэх мэт) болгон өөрчлөх нь **нейрон сүлжээний (ニューラルネットワーク)** рүү орох эхлэл болдог.

### シグモイド関数

$$h(x) = \frac{1}{1+e^{-x}} $$
Сигмойд функцын тэгшигтгэл.
### シグモイド関数とステップ関数の比較

Степ болон Сигмойд функцын ялгааг харицуулая.
![[Pasted image 20250918155836.png]]

Степ функц нь 0, 1 гэсэн хоёрхон утгыг гаргаж чадах бол сигмойд функц нь 0.731,  0.880 зэрэг бутархай утга гаргаж чадна гэдгээрээ ялгаатай. Нейрон сүлжээ үргэлжилсэн бодит утгуудыг гаргаж чаддаг гэсэн үг. 

Одоо ойролцоо байгаа зүйлийн тухайд гэвэл ерөнхийдөө ижил хэлбэрийг гаргаж байгаа. Бага оролт нам утга, их оролт өндөр утга гаргана.

### 非線形関数 (non linear function)

Идэвхжүүлэх функц нь ямар нэгэн оролт ороход ямар нэгэн утга буцаадаг хувиргах хэрэгсэл юм. Ингэхдээ гаралт нь оролттойгоо шууд пропорционал хамааралтай (тодорхой тоогоор үржүүлэн нөгөөгөө гаргадаг, хоёр хэмжигдэхүүний харьцаа тогтмол байх хамаарал) байх үед тухайн функц нь **шулуун функц**, үгүй бол **шулуун биш функц**.
$$(h(x) = cx, \quad c = \text{const}) \quad ? \quad \text{linear function} \quad : \quad \text{non-linear function} $$Нейрон сүлжээнд маань ашиглах идэвхжүүлэх функц нь **шулуун биш функц** байх ёстой.  Хэрвээ **шулуун функц** ашиглачихвал нейроны сүлжээний давхарга нь нэмэгдэх гэдэгт ямар ч ашиггүй болчихдог. Гүнзгийрэн давхарга нэмэгдэх тусам далд давхаргад өмнөх давхаргад хийгдсэнтэй ижил процесс явуулдаг нейрон заавал байж байдаг.
### ReLU (Rectified Linear Unit) 関数

![[Pasted image 20250918201421.png]]

Гаралт нь 0-ээс илүү байвал тэр утгаа буцаана, үгүй бол 0-ийг буцаана.
$$
h(x) = \begin{cases}
	x & (x > 0) \\
    0 & (x \leq 0) 
\end{cases}
$$
```python
def relu(x):
return np.maximum(0, x)
```

## 多次元配列の計算
 
Эдгээр $b, x_1, x_2, ... x_n, w_1, w_2, ... w_n$ үүдийг тооцоолох хэрэг одоо гарч ирнэ. 
Матриц дээр хийгдэх бүхий л үйлдлийг NumPy сан ашиглан хийж болно. 

Жин,  оролтуудыг тус тусад нь нэг матриц болгоод хооронд нь үржүүлчихвэл нийлбэрийг амархан бодно.

```python
>>> X = np.array([1, 2])
>>> X.shape
(2,)
>>> W = np.array([[1, 3, 5], [2, 4, 6]])
>>> print(W)
[[1 3 5]
[2 4 6]]
>>> W.shape
(2, 3)
```

![[Pasted image 20250918204306.png]]

```python
>>> Y = np.dot(X, W)
>>> print(Y)
[ 5 11 17]
```

Арай бодит нейрон сүлжээн дээр дахин жишээ авая.


