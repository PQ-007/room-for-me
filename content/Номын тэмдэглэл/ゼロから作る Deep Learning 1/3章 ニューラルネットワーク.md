***Perceptron  ашиглахад***:
- **Олон давхаргатай perceptron**-оор хэцүү бүтэц бүхий тооцооллын урсгалыг загварчилж болно.
- Гэвч **weight, bias** зэрэг параметрүүдийг нь зөв тохируулж өгөх хэрэгтэй.

> **Чухамдаа машин сургах гэдэг нь эдгээр параметрийг тохируулах тухай л зүйл.**

Гараар, хүний оролцоотой тохируулж ирсэн бол одоо энэхүү процессыг автоматжуулах хэрэг гарч ирэх бөгөөд нейрон сүлжээ нь үүний шийдэл юм. Тохирох жингийн параметрийг өгөгдөл дотроос өөрөө сурч чадах нь нейрон сүлжээний чухал шинж чанар.

Энэ бүлэгт нейрон сүлжээний үндсэн агуулга, түүний ялган таних үйл явц өрнүүлэх үеийн боловсруулалтын тухай голчлон авч үзнэ. 



## パーセプトロンからニューラルネットワークへ

Нейрон сүлжээ нь өмнө бүлэгт өгүүлсэн Perceptron-тай олон талаараа ижил. 

### ニューラルネットワークの例

Доорх зурагт нейрон сүлжээний жишээг харуулсан байна. Оролтын давхарга, дунд байрлах давхарга (далд давхарга), гаралтын давхарга гэсэн хэсгүүдтэй. Далд давхаргад нейрон нь оролтын болон гаралтын давхаргаас ялгаатай нь харагдахгүй учир ингэж нэрлэсэн. 

![[Neural Network Example.png]]

Хэлбэрийн хувьд нейронуудыг давхарлаж холбосон учир Perceptron-той ижил харагдана.
### パーセプトロンの復習

![[Perceptron Example.png | 3.2]]
$$
y = \begin{cases}
    0 & (b+w_1 x_1 + w_2 x_2 \leq 0) \\
    1 & (b+w_1 x_1 + w_2 x_2 > 0)
\end{cases}
$$
Perceptron нь дээрх тэгшитгэлийн дагуу ажилладаг. 
Энд $b$ нь bias, $w_1、w_2$ нь жин, $x_1、x_2$ нь оролтын утгууд. Жишээ зурагт bias-ыг дүрсэлж харуулаагүй бөгөөд 1 гэдэг тогтмол оролттой нэмэлт  нейрон байдлаар нэмж харуулж болно. Орох утга нь тогтмол 1-тэй тэнцүү байх учир бусад нейроноос ялгах үүднээс саарал өнгөтэй байгаа. Энэхүү нейроны хувьд $b$ нь жин шиг үүрэгтэй (тохируулж болохуйц параметр гэдгийг нь илэрхийлэх үүднээс  холбоосон дээр дүрслэгдсэн).
$$z=(w1​⋅x1​)+(w2​⋅x2​)+b$$ Энэ бол bias-ыг ($b$) жинтэй($w$) оролттой адил авч үзсэн загвар юм. Энэ нь: $$z=(w1⋅x1)+(w2⋅x2)+(b⋅1) $$
![[Perceptron with bias.png]]

Perceptron-ны тэгшитгэлийг хялбарчилж өөр хэлбэрт бичье. Хялбарчлахын тулд тухайн оролт бүрт бодогдон  гарч буй нийлбэр нь 0-ээс их байх үед 1, эсрэг тохиолдолд 0-ийг гаргах хэсгийг нэг функц болгоё. Энэ функцыг $h(x)$ гээд шинээр бичвэл:

$$
y = h(b + w_1x_1 + w_2x_2)
$$
$$
h(x) = \begin{cases}
    0 & (x \leq 0) \\
    1 & (x > 0)
\end{cases}
$$
Эхний тэгшитгэл нь оролтын дохионуудын нийлбэрийг  $h(x)$ гэдэг функцт оруулж буцааж буй утгыг нь $y$-т авч байгааг илэрхийлнэ. Дараагийн тэгшитгэл нь $h(x)$ гэдэг функц нь оролт нь эерэг үед 1-ийг буцааж, сөрөг үед 0-ийг буцаадаг функц гэдгийг харуулж байна.

### 活性化関数の登場 (Activation function)

Дээр дурдсан $h(x)$ шиг оролтын дохионуудын үржвэр нийлбэрийг хүлээн авч гаралтын дохионд хувиргадаг функцуудыг **Идэвхжүүлэх функц (Activation function)** гэдэг.
Энэ функц дохионуудын нийлбэрийг хэр зэрэг идэвхжүүлэх вэ? Өөрөөр хэр зэрэг хүчтэй дохиог цааш дамжуулах вэ? гэдгийг шийдвэрлэх үүрэгтэй.

Өмнөх тэгшитгэлүүдэд эхлээд оролтын дохионы утгууд тэдгээрийн харгалзах жингийн үржвэрүүдийн нийлбэрүүдийг олж дараа нь энэ нийлбэрээ идэвхжүүлэх функцээр хувиргах 2 шаттай боловсруулалт явуулж байсан. Үүнийг илүү цэвэрхэн ойлгомжтойгоор бичвэл:
$$a = b + w_1x_1 + w_2x_2$$
$$y=h(a)$$
болно. Дээрхийг зургаар харуулвал

![[Perceptron with activation function.png]]

  Энэхүү идэвхжүүлэх функц нь Perceptron-оос нейроны сүлжээ рүү урагшлах холбох гүүр болно.

## 活性化関数

- Perceptron дахь идэвхжүүлэх функц нь **степ функц (step function)** байдаг.
    
- Энэ функц нь оролтын утгыг тодорхой нэг **threshold (閾値)**-оос хамааран 0 эсвэл 1 болгон гаргадаг.
    
- Хэрэв степ функцийн оронд өөр төрлийн активэйшн функц ашиглавал яах вэ?
    
- Үнэндээ, идэвхжүүлэх функцийг степээс өөр (жишээ нь сигмойд, ReLU гэх мэт) болгон өөрчлөх нь **нейрон сүлжээний (ニューラルネットワーク)** рүү орох эхлэл болдог.

### シグモイド関数 (sigmoid function)

$$h(x) = \frac{1}{1+e^{-x}} $$
Сигмойд функцын тэгшигтгэл.

>**関数**は、何か入力を与えれば、何らかの出力が返される変換器です。
>**Функц** нь ямар нэгэн оролт өгөхөд ямар нэгэн  гаралт буцаадаг хувиргагч юм.
### シグモイド関数とステップ関数の比較


Степ болон Сигмойд функцын ялгааг харицуулая.
![[Step VS Sigmoid.png]]

Степ функц нь 0, 1 гэсэн хоёрхон утгыг гаргаж чадах бол сигмойд функц нь 0.731,  0.880 зэрэг бутархай утга гаргаж чадна гэдгээрээ ялгаатай. Нейрон сүлжээ үргэлжилсэн (滑らかさ) бодит утгуудыг гаргаж чаддаг гэсэн үг. 

Төсөөтэй ажиглагдаж байгаа зүйл гэвэл ерөнхийдөө ижил хэлбэрийг гаргаж байгаа. Бага оролт нам утга, их оролт өндөр утга гаргана.

### 非線形関数 (non linear function)

Гаралт нь оролттойгоо шууд пропорционал хамааралтай (тодорхой тоогоор үржүүлэн нөгөөгөө гаргадаг, хоёр хэмжигдэхүүний харьцаа тогтмол байх хамаарал) байх үед тухайн функц нь **шулуун функц**, үгүй бол **шулуун биш функц**.
$$(h(x) = cx, \quad c = \text{const}) \quad ? \quad \text{linear function} \quad : \quad \text{non-linear function} $$Нейрон сүлжээнд маань ашиглах идэвхжүүлэх функц нь **шулуун биш функц** байх ёстой.  Хэрвээ **шулуун функц** ашиглачихвал нейроны сүлжээний давхарга нь нэмэгдэх гэдэгт ямар ч ашиггүй болчихдог. Гүнзгийрэн давхарга нэмэгдэх тусам далд давхаргад өмнөх давхаргад хийгдсэнтэй ижил процесс явуулдаг нейрон заавал байж байдаг.
### ReLU (Rectified Linear Unit) 関数

![[ReLU.png]]

Гаралт нь 0-ээс илүү байвал тэр утгаа буцаана, үгүй бол 0-ийг буцаана.
$$
h(x) = \begin{cases}
	x & (x > 0) \\
    0 & (x \leq 0) 
\end{cases}
$$
```python
def relu(x):
return np.maximum(0, x)
```

## 多次元配列の計算
 
Эдгээр $b, x_1, x_2, ... x_n, w_1, w_2, ... w_n$ үүдийг тооцоолох хэрэг одоо гарч ирнэ. 
Матриц дээр хийгдэх бүхий л үйлдлийг NumPy сан ашиглан хийж болно. 

Жин,  оролтуудыг тус тусад нь нэг матриц болгоод хооронд нь үржүүлчихвэл нийлбэрийг амархан бодно.

![[Matrix Multipication.png]]

```python
>>> X = np.array([1, 2])
>>> X.shape
(2,)
>>> W = np.array([[1, 3, 5], [2, 4, 6]])
>>> print(W)
[[1 3 5]
[2 4 6]]
>>> W.shape
(2, 3)
```

![[NumPy Implementation on NN.png]]

```python
>>> Y = np.dot(X, W)
>>> print(Y)
[ 5 11 17]
```

Арай бодит нейрон сүлжээн дээр дахин жишээ авъя.

![[NN example.png]]

....

## 出力層の設計

Нейрон сүлжээг **ялгах (classification)**, болон **багасгах (regression)** гэсэн 2 даалгаварт ашиглаж болно. 

### 恒等関数とソフトマックス関数

**Таних функц (Identity function)** гэж орж ирсэн утгыг тэр чигээр нь гаргадаг функц юм.
Гаралтын давхаргад энэ функцийг ашиглана. 

![[Identity Function Example.png]]

Ялгаж ангиллах даалгаварт ашигладаг нь харин **Softmax** юм. 

$$ y_k = \frac{e^{a_k}}{\sum \limits_{i=1}^{n} e^{a_i}} $$
Энд гаралтын давхарга нийтдээ **n** ширхэг байна гэж үзвэл, **k** дахь гаралт $y_k$-ийг тооцоолох томъёог үзүүлж байна. Томъёод үзүүлснээр, Softmax функцийн тооны дээд хэсэг нь оролтын дохион $a_k$-ийн экспоненциал, доод хэсэг нь бүх оролтын дохионуудын экспоненциалын нийлбэрээр бүрддэг.

![[Softmax.png]]

### ソフトマックス関数の実装上の注意

Softmax ашиглах үед анхны томьёо компьютерийн тооцоолж чадахгүй хэт их тоо руу хөтөлдөг. Тиймээс дараах  сайжруулсан томьёог ашигладаг.

$$ y_k = \frac{e^{a_k}}{\sum \limits_{i=1}^{n} e^{a_i}} = \frac{Ce^{a_k}}{C\sum \limits_{i=1}^{n} e^{a_i}} = \frac{e^{a_k+logC}}{\sum \limits_{i=1}^{n} e^{a_i+logC}} = \frac{e^{a_k+C^{'}}}{\sum \limits_{i=1}^{n} e^{a_i+C^{'}}} $$
### ソフトマックス関数の特徴

Softmax-ийн гаралтын утгууд нь 0-ээс 1-ийн хоорондох бутархай тоонууд байх ба тэдгээрийн нийлбэр нь 1-тэй тэнцүү байдаг. Тийм болохоор гаргалтуудыг хувиар илэрхийлэх боломжтой болгодог. Softmax ашиглахад гаралтуудын хоорондыг харьцаа өмнө байснаараа хадгалагддаг. Ийм учраас Softmax ашиглахгүй байсан ч хамгийн их утгатайг нь хариу болгон сонгож чадна. Softmax ашигладаг нь сургалт явуулах үед хэрэгтэй учраас тэр.
### 出力層のニューロンの数

Гаралтын давхаргын нейронуудын тоог шийдэхдээ тухайн даалгаварын шинж чанарт тохируулан тогтоох шаардлагатай. Хэрэв ангилах даалгавар шийдэх гэж байгаа бол гаралтын давхаргын нейронуудын тоог ангилах гэж буй ангиллын тоонд харгалзуулан тохируулах нь түгээмэл. Жишээлбэл, нэгэн оролтын зургийн хувьд, тухайн зураг нь 0–9 хүртэлх тоонуудын аль нэг болохыг таамаглах асуудал буюу **10 төрлийн сонголттой** энэ нөхцөлд гаралтын давхаргын нейронуудыг 10 болгон тохируулна.

![[Classification Problem NN.png]]

**Сургалтын үе шат аль хэдийн дууссан** гэж төсөөлөе. Өөрөөр хэлбэл, моделио сургачихсан бөгөөд сурсан параметрүүд (жингүүд, bias) бидэнд байгаа. Тиймээс дахин сургах шаардлагагүй.

Бид зөвхөн **таамаглах процесс** буюу сурсан моделио ашиглан шинэ, мэдэгдээгүй өгөгдөл дээр ангилал хийх процессыг хэрэгжүүлэх болно.

Энэ таамаглах процессыг мөн **forward propagation (順方向伝播)** гэж нэрлэдэг. Учир нь өгөгдөл нь **сүлжээгээр урагш урсах** байдлаар тооцоологддог:

1. Оролтын дүрс анхны давхаргад орно.
    
2. Давхарга бүр сурсан жингүүд ба идэвхжүүлэх функцуудыг ашиглан гаралт тооцно.
    
3. Эцэст нь гаралтын давхарга тухайн оролт ямар тоо байж болох магадлалын оноонуудыг гаргана.

Товчхондоо:

- **Сургалтын үе шат** = олон жишээгээр моделио сургаж параметрүүдийг тааруулах
    
- **Таамаглах үе шат / Forward propagation** = сурсан моделиор оролтыг урагш дамжуулж гаралтыг авах (зураг дээр ямар тоо байгааг таах).

гэсэн 2 үе шатанд машин сургалтын үйл явц өрнөдөг.

### バッチ処理

.....

## まとめ

- Нейрон сүлжээнд идэвхжүүлэх функц болгон **сигмоид функц**, **ReLU функц** зэрэг удаан,  тасралтгүй, бага багаар өөрчлөгддөг функцуудыг ашигладаг.  
	
- NumPy-ийн олон хэмжээст массивыг зөв ашигласнаар нейрон сүлжээн дээр өрнөх тооцооллуудыг илүү үр дүнтэй хурдан гүйцэтгэх боломжтой. 
	
- Машин сургалтын шийдвэрлэж буй даалгавруудыг ерөнхийд нь **регрессийн асуудал** болон **ангиллын асуудал** гэж хоёр хуваадаг.  
	
- Гаралтын давхаргад ашиглах идэвхжүүлэх функц нь, регрессийн даалгаварын үед **тэнцүү (identity) функц**, ангиллын даалгаварын үед **Softmax функц** байх нь түгээмэл. 
	
- Ангиллын даалгавар дээр гаралтын давхаргын нейронуудын тоог ангилах гэж буй төрлүүдийн тоотой ижил байхаар тохируулдаг.
	
- Оролтын өгөгдлийн бөөгнөрлийг **batch** гэж нэрлэдэг бөгөөд batch нэгжээр таамаглах процессыг хийх нь тооцооллыг хурдан болгодог.